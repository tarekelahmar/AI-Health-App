# Insight Quality Testing Guide

## Overview

This guide explains how to test and validate the quality of insights generated by the AI Health App. Quality testing ensures that insights are:
- **Statistically valid**: Based on sound statistical methods
- **Medically reasonable**: Values are within plausible ranges
- **Data-sufficient**: Based on enough data points
- **Consistent**: Same data produces similar insights
- **Reliable**: Correlations and trends are trustworthy

## Quality Dimensions

### 1. Data Sufficiency
- **Minimum window**: At least 14 days of data
- **Minimum data points**: At least 10 points per metric
- **Correlation reliability**: Correlations need n >= 10 and |r| >= 0.3

### 2. Statistical Validity
- Correlation coefficients in valid range [-1, 1]
- Strength interpretation matches correlation value
- Sample sizes are adequate
- No NaN or infinite values

### 3. Trend Accuracy
- Trend direction matches trend_delta
- Trend values are valid ("improving", "worsening", "stable", "unknown")
- Large deltas don't correspond to "stable" trends

### 4. Correlation Reliability
- `is_reliable` flag matches actual reliability
- Direction matches correlation sign (positive/negative)
- Sample size and correlation strength are adequate

### 5. Medical Reasonableness
- Values are within medically plausible ranges
- Sleep duration: 3-12 hours
- HRV: 20-200ms
- Activity: 0-600 minutes/day
- Stress score: 0-10

## Running Quality Tests

### Unit Tests
```bash
# Test quality validation functions
pytest tests/unit/engine/test_insight_quality.py -v
```

### Integration Tests
```bash
# Test quality with real data
pytest tests/integration/test_insight_quality_integration.py -v
```

### All Quality Tests
```bash
# Run all quality-related tests
pytest tests/ -k "quality" -v
```

## Using Quality Assessment

### Programmatic Assessment

```python
from app.utils.insight_quality import assess_insight_quality
from app.engine.reasoning.insight_generator import InsightEngine

# Generate an insight
engine = InsightEngine(...)
insight = engine.generate_sleep_insights(user_id=1, window_days=30)

# Assess quality
report = assess_insight_quality(insight)

# Check if passed
if report.passed:
    print(f"✅ Insight quality: {report.score.overall:.2f}")
    # Use the insight
else:
    print(f"❌ Insight quality too low: {report.score.overall:.2f}")
    print("Issues:", report.score.issues)
    # Don't use the insight or flag for review
```

### Quality Report Structure

```python
report = assess_insight_quality(insight)

# Overall score (0.0 to 1.0)
report.score.overall  # e.g., 0.85

# Dimension scores
report.score.data_sufficiency      # 0.0 to 1.0
report.score.statistical_validity # 0.0 to 1.0
report.score.trend_accuracy       # 0.0 to 1.0
report.score.correlation_reliability # 0.0 to 1.0

# Issues found
report.score.issues  # List of strings describing problems

# Recommendations
report.recommendations  # List of improvement suggestions

# Pass/fail
report.passed  # True if overall >= 0.7
```

## Quality Thresholds

### Minimum Acceptable Quality
- **Overall score**: >= 0.7
- **Data sufficiency**: >= 0.7
- **Statistical validity**: >= 0.8

### High Quality
- **Overall score**: >= 0.9
- All dimension scores: >= 0.85
- No issues found

### Low Quality (Reject)
- **Overall score**: < 0.7
- Critical issues present (e.g., invalid statistics, no data)

## Testing Scenarios

### 1. Test with Realistic Data
```python
# Create 30 days of realistic wearable data
for i in range(30):
    wearable_repo.create(
        user_id=1,
        device_type="fitbit",
        metric_type="sleep_duration",
        value=7.0 + random.uniform(-0.5, 0.5),
        unit="hours",
        timestamp=now - timedelta(days=i),
    )

insight = engine.generate_sleep_insights(user_id=1, window_days=30)
report = assess_insight_quality(insight)

assert report.passed is True
assert report.score.overall >= 0.7
```

### 2. Test with Insufficient Data
```python
# Create only 3 days of data
for i in range(3):
    wearable_repo.create(...)

insight = engine.generate_sleep_insights(user_id=1, window_days=30)
if insight:
    report = assess_insight_quality(insight)
    # Should fail or have low quality
    assert report.score.data_sufficiency < 0.7
```

### 3. Test Consistency
```python
# Generate insights twice with same data
insight1 = engine.generate_sleep_insights(user_id=1, window_days=30)
insight2 = engine.generate_sleep_insights(user_id=1, window_days=30)

from app.utils.insight_quality import compare_insights
comparison = compare_insights(insight1, insight2)

assert comparison["consistent"] is True
```

### 4. Test Edge Cases
- Empty data
- Single data point
- Outliers
- Missing correlations
- Invalid trend values

## Quality Metrics Dashboard

You can create a dashboard to monitor insight quality:

```python
# Collect quality metrics
quality_scores = []
for user_id in user_ids:
    insight = engine.generate_sleep_insights(user_id, window_days=30)
    if insight:
        report = assess_insight_quality(insight)
        quality_scores.append({
            "user_id": user_id,
            "overall": report.score.overall,
            "passed": report.passed,
            "issues_count": len(report.score.issues),
        })

# Calculate statistics
avg_quality = sum(s["overall"] for s in quality_scores) / len(quality_scores)
pass_rate = sum(1 for s in quality_scores if s["passed"]) / len(quality_scores)
```

## Best Practices

### 1. Always Validate Before Using
```python
insight = engine.generate_sleep_insights(user_id=1)
report = assess_insight_quality(insight)

if not report.passed:
    # Log warning
    logger.warning(f"Low quality insight for user {user_id}: {report.score.overall}")
    # Optionally: don't persist or flag for review
    return None

# Only persist if quality is acceptable
engine.persist_insight(insight)
```

### 2. Monitor Quality Over Time
Track quality scores to detect degradation:
- Average quality per day/week
- Pass rate trends
- Common issues

### 3. Set Quality Gates
- **Development**: Warn if quality < 0.7
- **Staging**: Block insights with quality < 0.6
- **Production**: Block insights with quality < 0.7

### 4. User Feedback Loop
Collect user feedback on insights:
- "Was this insight helpful?"
- "Was this insight accurate?"
- Use feedback to improve quality thresholds

## Troubleshooting

### Low Data Sufficiency
**Problem**: `data_sufficiency < 0.7`
**Solutions**:
- Collect more data points
- Increase time window
- Use different aggregation methods

### Low Statistical Validity
**Problem**: `statistical_validity < 0.8`
**Solutions**:
- Review correlation calculations
- Check for data quality issues
- Validate statistical methods

### Inconsistent Insights
**Problem**: Same data produces different insights
**Solutions**:
- Check for non-deterministic operations
- Review data ordering
- Validate aggregation methods

## Future Enhancements

1. **Machine Learning Quality Model**: Train a model to predict insight quality
2. **A/B Testing**: Compare insights with different quality thresholds
3. **Automated Quality Monitoring**: Alert when quality drops
4. **Quality-Based Ranking**: Prioritize higher-quality insights
5. **Domain-Specific Validation**: Add medical reference ranges

## Example: Complete Quality Test

```python
def test_complete_quality_workflow(db):
    """Complete workflow: generate, assess, and validate insight"""
    # Setup
    engine = create_insight_engine(db)
    create_test_data(db, user_id=1, days=30)
    
    # Generate
    insight = engine.generate_sleep_insights(user_id=1, window_days=30)
    assert insight is not None
    
    # Assess
    report = assess_insight_quality(insight)
    
    # Validate
    assert report.passed is True, f"Quality too low: {report.score.overall}"
    assert report.score.overall >= 0.7
    assert len(report.score.issues) == 0
    
    # Check specific dimensions
    assert report.score.data_sufficiency >= 0.7
    assert report.score.statistical_validity >= 0.8
    
    # Only persist if quality is good
    if report.passed:
        engine.persist_insight(insight)
    
    return insight, report
```

---

**Remember**: Quality testing is critical for medical insights. Always validate before presenting insights to users.

